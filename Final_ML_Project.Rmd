---
title: "Predicting Vacation vs Business Travel Using Machine Learning"
author: "Andrew Burson, James Bressani, Matthew Humphreys, Sneh Patel"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: cosmo
  word_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 10, fig.height = 6)
set.seed(11)
```

# Executive Summary

This analysis predicts whether domestic flights are for vacation or business travel using machine learning. We address class imbalance through SMOTE resampling and enhance predictions through feature engineering. Our best model achieves over 93% accuracy with balanced performance across both classes.

# Load Libraries

```{r libraries}
library(tidyverse)
library(caret)
library(pROC)
library(randomForest)
library(kernlab)
```

# Data Preparation

```{r load_data}
air <- read.csv("Airfares.csv", stringsAsFactors = FALSE)

cat("Dataset:", nrow(air), "observations,", ncol(air), "variables\n")
cat("\nClass distribution:\n")
table(air$VACATION)
prop.table(table(air$VACATION))
```

## Train-Test Split

```{r split}
idx <- createDataPartition(air$VACATION, p = 0.60, list = FALSE)
train_df <- air[idx, ]
test_df <- air[-idx, ]

for (nm in c("VACATION", "SW", "SLOT", "GATE")) {
  train_df[[nm]] <- factor(train_df[[nm]])
  test_df[[nm]] <- factor(test_df[[nm]])
}

train_df$VACATION <- relevel(train_df$VACATION, ref = "Yes")
test_df$VACATION <- relevel(test_df$VACATION, ref = "Yes")

cat("Training set:", nrow(train_df), "| Test set:", nrow(test_df), "\n")
```

## Feature Engineering

```{r feature_engineering}
engineer_features <- function(df) {
  df %>%
    mutate(
      INCOME_RATIO = S_INCOME / (E_INCOME + 1),
      POP_RATIO = S_POP / (E_POP + 1),
      DISTANCE_CAT = case_when(
        DISTANCE < 500 ~ "Short",
        DISTANCE >= 500 & DISTANCE < 1500 ~ "Medium",
        DISTANCE >= 1500 ~ "Long"
      ),
      DISTANCE_CAT = factor(DISTANCE_CAT, levels = c("Short", "Medium", "Long")),
      FARE_PER_MILE = FARE / (DISTANCE + 1),
      AVG_INCOME = (S_INCOME + E_INCOME) / 2,
      AVG_POP = (S_POP + E_POP) / 2
    )
}

train_eng <- engineer_features(train_df)
test_eng <- engineer_features(test_df)
```

# Model Training

## Training Configuration

```{r train_config}
# Standard CV
ctrl_standard <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# CV with SMOTE
ctrl_smote <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "smote"
)
```

## Model Formulas

```{r formulas}
predictors_base <- c("FARE", "DISTANCE", "HI", "S_INCOME", "E_INCOME", 
                     "COUPON", "S_POP", "E_POP", "SW")
formula_base <- as.formula(paste("VACATION ~", paste(predictors_base, collapse = " + ")))

predictors_enh <- c(predictors_base, "INCOME_RATIO", "POP_RATIO", 
                    "DISTANCE_CAT", "FARE_PER_MILE", "AVG_INCOME", "AVG_POP")
formula_enh <- as.formula(paste("VACATION ~", paste(predictors_enh, collapse = " + ")))
```

## Baseline Models

```{r baseline_models, cache=TRUE}
cat("Training baseline models...\n")

mod_glm_base <- train(formula_base, data = train_df, method = "glm",
                      family = binomial(), metric = "ROC", trControl = ctrl_standard)

mod_svm_base <- train(formula_base, data = train_df, method = "svmRadial",
                      tuneLength = 5, metric = "ROC", trControl = ctrl_standard)

mod_rf_base <- train(formula_base, data = train_df, method = "rf",
                     tuneLength = 5, metric = "ROC", trControl = ctrl_standard)
```

## SMOTE-Enhanced Models

```{r smote_models, cache=TRUE}
cat("Training SMOTE-enhanced models...\n")

mod_glm_smote <- train(formula_base, data = train_df, method = "glm",
                       family = binomial(), metric = "ROC", trControl = ctrl_smote)

mod_svm_smote <- train(formula_base, data = train_df, method = "svmRadial",
                       tuneLength = 5, metric = "ROC", trControl = ctrl_smote)

mod_rf_smote <- train(formula_base, data = train_df, method = "rf",
                      tuneLength = 5, metric = "ROC", trControl = ctrl_smote)
```

## Enhanced Models (SMOTE + Features)

```{r enhanced_models, cache=TRUE}
cat("Training enhanced models...\n")

mod_glm_enh <- train(formula_enh, data = train_eng, method = "glm",
                     family = binomial(), metric = "ROC", trControl = ctrl_smote)

mod_svm_enh <- train(formula_enh, data = train_eng, method = "svmRadial",
                     tuneLength = 5, metric = "ROC", trControl = ctrl_smote)

mod_rf_enh <- train(formula_enh, data = train_eng, method = "rf",
                    tuneLength = 5, metric = "ROC", trControl = ctrl_smote)
```

# Evaluation

```{r eval_functions}
evaluate_model <- function(model, test_data, model_name) {
  pred_prob <- predict(model, test_data, type = "prob")$Yes
  pred_class <- predict(model, test_data, type = "raw")
  cm <- confusionMatrix(pred_class, test_data$VACATION, positive = "Yes")
  roc_obj <- roc(test_data$VACATION, pred_prob, levels = c("No", "Yes"))
  
  tibble(
    Model = model_name,
    CV_ROC = max(model$results$ROC, na.rm = TRUE),
    Test_Accuracy = cm$overall["Accuracy"],
    Test_Sensitivity = cm$byClass["Sensitivity"],
    Test_Specificity = cm$byClass["Specificity"],
    Test_ROC = as.numeric(auc(roc_obj)),
    Test_F1 = cm$byClass["F1"]
  )
}
```

## Results

```{r results}
results_all <- bind_rows(
  evaluate_model(mod_glm_base, test_df, "Logistic (Baseline)") %>% mutate(Approach = "Baseline"),
  evaluate_model(mod_svm_base, test_df, "SVM (Baseline)") %>% mutate(Approach = "Baseline"),
  evaluate_model(mod_rf_base, test_df, "Random Forest (Baseline)") %>% mutate(Approach = "Baseline"),
  evaluate_model(mod_glm_smote, test_df, "Logistic (SMOTE)") %>% mutate(Approach = "SMOTE"),
  evaluate_model(mod_svm_smote, test_df, "SVM (SMOTE)") %>% mutate(Approach = "SMOTE"),
  evaluate_model(mod_rf_smote, test_df, "Random Forest (SMOTE)") %>% mutate(Approach = "SMOTE"),
  evaluate_model(mod_glm_enh, test_eng, "Logistic (Enhanced)") %>% mutate(Approach = "Enhanced"),
  evaluate_model(mod_svm_enh, test_eng, "SVM (Enhanced)") %>% mutate(Approach = "Enhanced"),
  evaluate_model(mod_rf_enh, test_eng, "Random Forest (Enhanced)") %>% mutate(Approach = "Enhanced")
) %>% select(Approach, Model, everything())

knitr::kable(results_all %>% mutate(across(where(is.numeric), ~round(., 3))),
             caption = "Model Performance Comparison")
```

## Performance Gains

```{r improvements}
baseline <- results_all %>% filter(Approach == "Baseline") %>%
  select(Model, Base_Acc = Test_Accuracy, Base_Sens = Test_Sensitivity)

enhanced <- results_all %>% filter(Approach == "Enhanced") %>%
  select(Model, Enh_Acc = Test_Accuracy, Enh_Sens = Test_Sensitivity)

gains <- baseline %>%
  left_join(enhanced, by = "Model") %>%
  mutate(
    Acc_Gain = (Enh_Acc - Base_Acc) * 100,
    Sens_Gain = (Enh_Sens - Base_Sens) * 100
  ) %>%
  select(Model, Base_Acc, Enh_Acc, Acc_Gain, Sens_Gain)

knitr::kable(gains %>% mutate(across(where(is.numeric), ~round(., 2))),
             caption = "Baseline vs Enhanced Improvements",
             col.names = c("Model", "Baseline Acc", "Enhanced Acc", "Acc Gain (%)", "Sens Gain (%)"))
```

# Visualizations

## Accuracy Comparison

```{r accuracy_plot}
results_all %>%
  mutate(Model = str_remove(Model, " \\(.+\\)")) %>%
  ggplot(aes(x = Model, y = Test_Accuracy, fill = Approach)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.1f%%", Test_Accuracy * 100)),
            position = position_dodge(0.9), vjust = -0.5, size = 3.5) +
  scale_fill_manual(values = c("Baseline" = "#E74C3C", "SMOTE" = "#F39C12", "Enhanced" = "#27AE60")) +
  labs(title = "Test Accuracy Comparison", y = "Test Accuracy", x = "") +
  theme_minimal() +
  theme(legend.position = "top", plot.title = element_text(face = "bold"))
```

## Sensitivity Comparison

```{r sensitivity_plot}
results_all %>%
  mutate(Model = str_remove(Model, " \\(.+\\)")) %>%
  ggplot(aes(x = Model, y = Test_Sensitivity, fill = Approach)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.2f", Test_Sensitivity)),
            position = position_dodge(0.9), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("Baseline" = "#E74C3C", "SMOTE" = "#F39C12", "Enhanced" = "#27AE60")) +
  labs(title = "Sensitivity (Vacation Trip Detection)", y = "Sensitivity", x = "") +
  theme_minimal() +
  theme(legend.position = "top", plot.title = element_text(face = "bold"))
```

## Variable Importance

```{r var_importance}
best <- results_all %>% arrange(desc(Test_Accuracy)) %>% slice(1)

best_model <- if(best$Approach == "Baseline") {
  if(grepl("Random Forest", best$Model)) mod_rf_base
  else if(grepl("SVM", best$Model)) mod_svm_base
  else mod_glm_base
} else if(best$Approach == "SMOTE") {
  if(grepl("Random Forest", best$Model)) mod_rf_smote
  else if(grepl("SVM", best$Model)) mod_svm_smote
  else mod_glm_smote
} else {
  if(grepl("Random Forest", best$Model)) mod_rf_enh
  else if(grepl("SVM", best$Model)) mod_svm_enh
  else mod_glm_enh
}

imp <- varImp(best_model)
plot(imp, top = 10, main = paste("Variable Importance:", best$Model))
```

# Conclusions

```{r best_model}
best <- results_all %>% arrange(desc(Test_Accuracy)) %>% slice(1)

cat("Best Model:", best$Model, "\n")
cat("Approach:", best$Approach, "\n")
cat("Test Accuracy:", sprintf("%.1f%%", best$Test_Accuracy * 100), "\n")
cat("Test ROC:", sprintf("%.3f", best$Test_ROC), "\n")
cat("Sensitivity:", sprintf("%.3f", best$Test_Sensitivity), "\n")
cat("Specificity:", sprintf("%.3f", best$Test_Specificity), "\n")
```

## Key Findings

- **Class Imbalance**: SMOTE improved minority class detection across all models
- **Feature Engineering**: Income/population ratios and distance categories added predictive value
- **Best Performance**: Random Forest achieved excellent accuracy with balanced sensitivity/specificity
- **Practical Impact**: Model enables airlines to optimize pricing and service strategies based on trip purpose

# Export Results

```{r export}
write_csv(results_all, "model_results_comprehensive.csv")
write_csv(gains, "performance_improvements.csv")
saveRDS(best_model, "best_model.rds")
cat("Results exported successfully.\n")
```
